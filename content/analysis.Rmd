---
title: Analysis
description:
toc: true
featuredVideo:
featuredImage: https://upload.wikimedia.org/wikipedia/commons/7/77/Pebbleswithquarzite.jpg
draft: false
---

This comes from the file `content/analysis.Rmd`.

We describe here our detailed data analysis.

[second analysis page](/analysis2/)

## Motivation:

*Firstly, We want to know what factors affect and have relationships with the temperature. We want to test various variables such as “Year”, “Regions”, “CO2 emission”, “Coal”, “Gas”, “population”, “forest area”, and “Oil”. We are interested in answering whether those variables will lead to changes in the temperature and assume that these variables can affect or are related to temperature. We also want to compare these relationships and know which variable among those actually affects temperature most. Finally, we are wondering if our conclusions are valid and correct. 

```{r,warning=FALSE,message=FALSE,echo=FALSE}
library(tidyverse)
library(GGally)
library(patchwork) 
library(ggpmisc)
library(car)

source(
  here::here("static", "load_and_clean_data.R"),
  echo = FALSE)
```
## Motivation:
We first divide all the country into developed and developing, we want to check is there any difference in CO2 emission between these two categories.

## Visualization:
```{r}
all_developed = c("Switzerland","Norway","Iceland","Hong Kong","Australia","Denmark","Sweden","Ireland","Germany","Netherlands","Finland","Singapore","Belgium","New Zealand","Canada","Liechtenstein","Luxembourg","United Kingdom","Japan","South Korea","United States","Israel","Malta","Slovenia","Austria","United Arab Emirates","Spain","France","Cyprus","Italy","Estonia","Czechia","Greece","Poland","Bahrain","Lithuania","Saudi Arabia","Portugal","Latvia","Andorra","Croatia","Chile","Qatar","San","Slovakia","Hungary","Argentina","Turkey","Montenegro","Kuwait","Brunei","Russia","Romania","Oman","Bahamas","Kazakhstan","Trinidad and Tobago","Costa Rica","Uruguay","Belarus","Panama","Malaysia","Georgia","Mauritius","Serbia","Thailand")

CO2_clean_1 %>% filter(!Country %in% c('World','European Union (27)')) %>% mutate(developed = ifelse(Country %in% all_developed, 'Developed', 'Developing')) %>% group_by(Country) %>% 
  transmute(meanCO2 = mean(CO2emission,rm.na = TRUE), developed = developed) %>%
  distinct(Country, meanCO2, developed) %>%
  ggplot(aes(developed, meanCO2,color = developed)) + geom_boxplot() + ylim(-10,100) +
  geom_jitter(shape=16, position=position_jitter(0.2), alpha = .4) + 
  labs(title = 'Mean Emission of CO2 Form 1990-2016', x = 'Development Phase',
       y = 'Mean Emission of CO2')
```
This is the box plot that shows the mean emission of CO2 between developed and developing countries from 0 to 100 MtCO₂e.The reason that we only focus on the range smaller than 100 is that there are several country like China, United State, which has a very high CO2 emission around 4000 MtCO₂e.IF we do not set a limit, those countries will be count as outliers, and the trend of the CO2 emission is not clear at all. After we set up the range, the plot shows that the mean emission of CO2 for developed countries is higher than the developing countries. 



```{r}
TemChange_Year %>% mutate(developed = ifelse(Area %in% all_developed, 'Developed', 'Developing')) %>%
  filter(year %in% 1990:2018) %>% group_by(Area) %>% 
  transmute(meanTempChange = mean(change,rm.na = TRUE), developed = developed) %>%
  distinct(Area, meanTempChange, developed) %>%
  ggplot(aes(developed, meanTempChange, color = developed)) + geom_boxplot() +
  geom_jitter(shape=16, position=position_jitter(0.2), alpha = .4) + 
  labs(title = 'Mean Temperature Change Form 1990-2016', x = 'Development Phase',
       y = 'Mean Temperature Change')
```
We also plot the mean temperature change between these two categories, however, this is what we expect, in general, the CO2 emission will cause the temperature increase. In this way more CO2 emission will cause a more obvious changes in temperature. This plot confirms our conjecture， the developed countries have a more larger mean change in temperature than the developing countries. 

## Modeling:
In the previous paragraph, we simply analysis the data by dividing them into two categories, developed countries, and developing countries. The plot also confirms our conjecture, we then want to check whether this correlation is the same as before when analysis the data as a whole. Before directly apply a linear model on CO2 emission and temperature, we first want to visualize the trend of the temperature and CO2 emission separately by years. In order to get the relationship between year and temperature, we draw the figure of temperature and CO2emission with yeas individually, then we draw the plow between temperature and CO2emission to find whether or not there is a correlation between them.

###1.TEM vs Year
```{r}
Temp_Year_Mod = World_data %>% mutate(Year = as.numeric(Year)) %>% 
  lm(temperature ~ Year, data = .)

World_data %>% mutate(Year = as.numeric(Year) ) %>% ggplot(aes(Year, temperature)) +
  geom_point() + stat_poly_line(se = FALSE) + 
  stat_poly_eq(mapping = use_label(c('eq', 'adj.R2', 'p', 'AIC'))) +
  labs(title = 'Temperature over Time', y = '°C',
       caption = 'Compared with the Average Temperature in 20th Century')

broom::tidy(Temp_Year_Mod) %>% select(term, estimate, p.value) %>% 
  mutate(across(where(is.numeric), ~round(., 3))) %>% DT::datatable()
```
The is the plot between the temperature and years from 1990 to 2016. The temperature is not labeled in actual temperature in  Celsius, instead it is measured in the difference between that year and the average temperature in 20th Century. As we can see, the change in temperature  has been fluctuating between these 26 years. The overall all trend of temperature is increasing by years, we also we draw a linear line to represent the trend between these two variable. It is clear that the trend of temperature almost follow the model: temperature=0.018*year-35.5. The R square adjusted is 0.73, It means the data follow the model well and every one unit increase in year will also increase 0.018 towards the intercept when predicting the temperature.


###2.CO2 vs Year
```{r}
CO2_Year_Mod = World_data %>% mutate(Year = as.numeric(Year) ) %>% 
  lm(CO2emission/1000 ~ Year, data = .)

World_data %>% mutate(Year = as.numeric(Year)) %>% ggplot(aes(Year, CO2emission/1000)) +
  geom_point() + stat_poly_line(se = FALSE) + 
  stat_poly_eq(mapping = use_label(c('eq', 'adj.R2', 'p', 'AIC'))) + 
  labs(title = 'CO2 Emission over Time', y = 'CO2 Emission/1000',
       caption = 'CO2 Emission has been divided by 1000')

broom::tidy(CO2_Year_Mod) %>% select(term, estimate, p.value) %>% 
  mutate(across(where(is.numeric), ~round(., 3))) %>% DT::datatable()
```
The is the plot between the CO2 emission and years from 1990 to 2016. The unit of CO2 is labeled in 1000 MtCO₂e. As we can see, the change in CO2 emission  has been fluctuating between these 26 years, but the trend is clear. The overall all trend is increasing by years, we also we draw a linear line to represent the trend between these two variable. It is clear that the trend of CO2 emission almost follow the model: CO2 emission=0.546*year-1064.614. The R square adjusted is 0.96, It means the data is highly correlated with years and every one unit increase in year will also increase 0.546 towards the intercept when predicting the CO2 emission.

```{r}
acf(World_data$CO2emission, main="")
pacf(World_data$CO2emission, main="")
```
These two plot shows whether time series appears in this data. The autocorrelation plot is a geometric plot, the partial autocorrelation plot has one significant abnormal point, which indicates the CO2 emission follows a AR(1) model, and the CO2 emission of a certain year is highly correlated with the previous year's data. 


###3.TEM vs CO2
```{r}
Temp_CO2_Mod = World_data %>% lm(temperature ~ log(CO2emission), data = .) %>% summary()

World_data %>% ggplot(aes(log(CO2emission), temperature)) + geom_point() + 
  stat_poly_line(se = FALSE) + 
  stat_poly_eq(mapping = use_label(c('eq', 'adj.R2', 'p', 'AIC'))) +
  labs(title = 'Temperature by the Logarithm of CO2 Emission', y = 'Temperature',
       x = 'Logarithm of CO2 Emission')

broom::tidy(Temp_CO2_Mod) %>% select(term, estimate, p.value) %>% 
  mutate(across(where(is.numeric), ~round(., 3))) %>% DT::datatable()
```
The is the plot between the CO2 emission and temperature from 1990 to 2016. We are interesting to find the relationship between CO2 emission and temperature. As we can see, the change in temperature is increasing when the CO2 emission is also increasing from 22500 to around 26000. Then the temperature does not change as the CO2 emission continue increase. When the CO2 emission is around 31000, the temperature increase as the CO2 emission increase again. The overall all trend is there's a positive correlation between temperature and CO2 emission, we also we draw a linear line to represent the trend between these two variable. It is clear that the trend of these two variables almost follow the model: temperature=0.893*log(CO2 emission)-8.586. The R square adjusted is 0.68, It means 68% of the data can be explain by our model and and every one unit increase in log(CO2emission) will also increase 0.893 towards the intercept when predicting the temperature.


4.multiple linear regression
Since the correlation between CO2 emission and temperature is not that linear, we think there might be other parameters also have correlation with temperature. We draw a correlation matrix to explore the relationship between each parameters to determine which parameter can also been included into our model to predict the response variable.
```{r}
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
World_data %>% select(-Country_Name, -Year) %>% 
  rename(., Forest = forest_area, Pop = population, CO2 = CO2emission, Temp = temperature) %>% 
  select(Temp, everything()) %>% cor() %>%
  corrplot::corrplot(method = 'square', type = 'upper', diag = FALSE, tl.col = 'Black', tl.srt = 45,
                     tl.cex = .9 ,addCoef.col = "black", col = col(10000))
grid.text('The Correlation between Variables', .3 , .1)
```
This is the correlation matrix between each parameter, as we can see temperature have a strong negative correlation with forest area, except forest area, temperature have a positive relationship with all the other parameters. Since all the parameter has a high correlation with temperature, we decided to put all the parameters in a linear model to predict the trend of temperature.

```{r}
transform_World_data = World_data %>% mutate(forest_area = log(forest_area), population = log(population), 
                                         Gas = log(Gas), Oil = log(Oil),  Coal = log(Coal), 
                                         CO2emission = log(CO2emission)) %>% select(-Country_Name, -Year)

mult_full_model = lm(temperature ~ ., data = transform_World_data)
broom::tidy(mult_full_model) %>% select(term, estimate, p.value) %>% 
  mutate(across(where(is.numeric), ~round(., 3))) %>% DT::datatable()
```
The is the output of the linear model between the forest_area, population, Gas, Oil, Coal, CO2 emission, and temperature. The hypothesis of this linear model is all the parameters are not significant when predicting temperature. The p-values we get for forest area, population, Gas, oil, and CO2 emission are all above 0.05, most parameters are not significant indicates the model is not suitable to use as a prediction of temperature. In order to select the most accurate parameter in the model, we use backward step wise model selection.

```{r}
step(lm(temperature ~ log(forest_area) + log(population) + log(Gas) + log(Oil) +
                         log(Coal) + log(CO2emission), data = World_data), direction = 'both')

final_mod = lm(temperature ~ log(Coal) + log(CO2emission), data = World_data)
broom::tidy(final_mod) %>% select(term, estimate, p.value) %>% 
  mutate(across(where(is.numeric), ~round(., 3))) %>% DT::datatable()
```
Before we use the step wise model selection, we apply log transformation on all the parameter to make the trend of each parameter more normally distributed. This is the output from the model selection:  temperature=3.095*log(CO2 emission)-1.503*log(Coal)-15.503. This relationship means every one unit increase in log(CO2emission) will increase by 3.095 towards the intercept when predicting the temperature. However, every one unit increase in log(CO2emission) will decrease by 1.503 towards the intercept when predicting the temperature. The R square is 0.8 for this model, which means 80% of the data can be explained by our model. 

```{r}
temp.res=resid(lm(temperature ~ log(Coal) + log(CO2emission), 
    data = World_data))
cbind(World_data, temp.res) %>% ggplot(aes(temperature, temp.res)) + geom_point() +
  labs(title = 'Residuals Plot', x = 'Temperature', y = 'Residual')
```
We want to test how well our model perform in the prediction of temperature. We draw the residual plot of the model, from this plot we can see that the residual is not normally distributed, there's no point in the lower right corner, it seems to have it's own trend. This means our model does not fit the trend of change in temperature.

```{r}
predict_model<-(-5.154e-05*World_data$Coal+1.217e-04*World_data$CO2emission-1.157e+00)
plot(density(predict_model),lwd = 5, col = "Green",main = "Density between Obsearvation and Predicted Model of Temp")
lines(density(World_data$temperature), lwd = 2)
```
We also draw a density plot between observation and predicted model. The green line in the plot is our predicted model by plug in number into our model. The black line is the density line of observation. It is clear that most our predicted model follow the observation, but around 0.4, our predicted model have it's own trend. As the R square shows it's a good model, but the visualization plot of residual does not state it is fit. We came up with a question, whether multicollinearity problem appear in the multiple linear regression. We use VIF to test the correlation between explanatory variable. The output is 45 for both parameter. In general if VIF is larger than 10 there is a strong correlation between explanatory variables, which will cause over fitting towards the model. This is why our R square is high but the residual plot does not show a well goodness of fit.

5.ridge regression
As we state before, there are multicollinearity problem appear in the multiple linear regression, which will have the problem of over fitting when predicting the response variable. In order to make our estimation more accurate. We apply the ridge regression to reduce the influence of multicollinearity.Ridge regression, however, make a trade-off between bias and variance in prediction. By introducing a relatively small bias, we expect a large reduction in the variance, and thus in the mean-squared error.

As λ grows larger, the coefficients (as well as prediction variances) decrease, while the bias increases. Thus we have to select a λ to make a trade-off, so as to control the overall prediction error. 
```{r}
x = as.matrix(transform_World_data[,1:6])
y = as.matrix(transform_World_data[,7])

#glmnet::glmnet(x, y, alpha = 0)
#plot(MASS::lm.ridge(temperature ~ .,data = transform_World_data, lambda = seq(0, .1, .001)))
MASS::select(MASS::lm.ridge(temperature ~ .,data = transform_World_data, lambda = seq(0, .1, .001))) 
ridge_mod = ridge::linearRidge(temperature ~ .,data = transform_World_data, lambda = 0.1)

cbind(tibble(term = c('(Intercept)', 'Forest Area', 'Population', 'Gas','Oil','Coal',
                     'CO2 Emission')),as.tibble(summary(ridge_mod)$summaries$summary1$coefficients)) %>%
  rename(., p.value = `Pr(>|t|)`) %>% select(term, Estimate, p.value) %>% 
  mutate(across(where(is.numeric), ~round(., 3))) %>% DT::datatable()


(Rsquare = sum((predicted - mean(y))^2)/sum((y - mean(y))^2))


```
After using the ridge regression, we choose to use the lambda=0.1, under this assumption, and all the parameter is using log transformation to reach normality. we come up with a table of the summary of the ridge regression. The output is quite different from the output that we get in the multiple linear regression. Instead of only have coal and CO2emission, we have 6 parameters here, which are "Forest area", "Population", "Gas", "Oil", "Coal", and "CO2emission". By checking the p-value of these parameter, only the p-value for "Coal" is larger than 0.05, which is not significant in this model. So our final model is Temperature=-0.13*log(Forest area)+0.317*log(Population)+0.111*log(Gas)+0.46*log(Oil)+0.14*log(CO2 Emission)-9.05. We also calculate the R square of the model is 0.709, which means around 71% of the data can be explained by our ridge model. The coefficient of "Forest area" is -0.13, which mean every one unit decrease in the log(Forest area) will increase 0.13 towards the intercept to predict the response variable. The coefficient of other variables have the same meaning as the "Forest area" one I just explained.  
```{r}
predicted = as.matrix(cbind(rep(1, 27),transform_World_data[,1:6])) %*% coef(ridge_mod)
cbind(transform_World_data, predicted) %>% ggplot() + geom_point(aes(temperature, predicted))

```
In order to test the goodness of fit for our ridge model, we plot our predicted model with the observation data of temperature, this plot shows there is a positive linear correlation between the observation data and our predicted model. This plot confirmed our model did a good estimation on predicting the temperature, nearly all the points follow the positive relationship.

```{r}
transform_World_data %>% cbind(predicted) %>% mutate(Index = 1990:2016) %>% 
  ggplot() + geom_smooth(aes(Index, temperature), color = 'blue', se = FALSE) + 
  geom_smooth(aes(Index, predicted), color = 'red', se = FALSE)+ xlab("year")
```
Another way to test the goodness of fit of our model is draw the model and observation data together. In this plot, the blue line is the observation data, and the red line is our predicted model. Over all our model fit the trend of the observation well, although the original data is not as linear as we predicted, our model fit the trend of the temperature well. 



-------

